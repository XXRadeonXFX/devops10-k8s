Session 1:
-----------

1.) The "Why Kubernetes?" - Limitations of Docker Alone


Docker's Strengths:

    - Dockerfile: A blueprint or script containing instructions to build a Docker image. It specifies everything needed for your application (e.g., an e-commerce website) to run.
    
    - Image: A read-only template created from a Dockerfile. It's a build-time construct. Think of it as a packaged application with all its dependencies.
    
    - Container: A runnable instance of an image. It's a runtime construct. Think of it as a running process based on the image.
    
Challenges with Docker in Production (without an orchestrator):
    
    - Multi-Containers: While docker-compose is excellent for defining and running multi-container applications on a single Docker host, it's not designed for managing applications across a cluster of machines.

    - Cluster Management: Coordinating and managing containers across multiple machines.

    - Scaling: Manual Scaling is Tedious and Error-Prone: If your application needs more instances due to increased load, you'd have to manually docker run new containers. If these need to be on different servers, you'd SSH into each, pull the image, run the container, and configure networking. This doesn't scale for dozens or hundreds of instances.

    - Load Balancing: How do you distribute incoming traffic evenly across your multiple container instances? You'd need to manually configure an external load balancer (like Nginx, HAProxy, or a cloud load balancer) and update its configuration every time you add or remove a container instance. This is slow and prone to misconfiguration.

    - Resource Allocation: How do you decide which server to run a new container on to best utilize available CPU and memory? This becomes a manual, complex decision process without an orchestrator intelligently scheduling workloads.

    - No Automated Horizontal Scaling: Docker itself has no built-in mechanism to automatically increase or decrease the number of container instances based on load (e.g., CPU utilization or incoming requests).
    
    - Ephemeral Nature & Self-Healing: Docker's restart policies (always, unless-stopped, etc.) can restart a container if the container process itself exits. But what if the container is running, but the application inside it is frozen, in a deadlock, or consistently returning errors? Docker alone doesn't have sophisticated application-level health checks to detect this state and take corrective action

    - Service Discovery: How do containers find and talk to each other when their IPs can change due to restart?

    - Automated Rollouts & Rollbacks: Safely deploying new versions of your application without downtime.

    - Declarative State Management: Defining the desired state of your application (e.g., "I want 5 instances of this web server running version 1.2") and having a system work to achieve and maintain that state.


All the above problems are resolved using Container orchestrator like Kubernetes

-----

2.) Managing Applications at Scale using Kubernetes

    - Real world Production Workloads e.g., amazon.com often:
        
        - Run on a cluster of servers to handle load and ensure fault tolerance:

            Handling Load (Scalability): A single server, no matter how powerful, has finite resources (CPU, RAM, network bandwidth). As user traffic and data processing needs grow, the application load can exceed the capacity of one machine. A cluster allows the workload to be distributed across multiple servers.

            Ensuring Fault Tolerance (High Availability): Hardware fails, software crashes, networks glitch. If your entire application runs on a single server, any failure on that server means your application goes down. By running on a cluster, if one server (or "node") fails, the others can continue to operate and potentially take over the workload of the failed node, minimizing or preventing downtime.
        
        - Are built as a collection of microservices (e.g., amazon.com might have 40+ services like checkout, order, frontend, API gateway):

            Complexity Management: Large, monolithic applications become increasingly difficult to understand, develop, maintain, and update. Breaking them down into smaller, independent services (microservices) makes each part more manageable.

            Independent Deployment & Scaling: Each microservice (e.g., checkout-service, order-service, user-profile-service, product-catalog-service, frontend-ui, api-gateway for an e-commerce site) can be developed, deployed, and scaled independently. If the product-catalog-service needs more resources due to high browsing activity, it can be scaled up without affecting the checkout-service.
    
        - These microservices need to communicate with each other (e.g., frontend talks to an API, which talks to a backend service etc.)

            Functional Decomposition: Microservices are individual pieces of a larger puzzle. To fulfill a complete user request or business process, they need to collaborate.

            Data Flow and Orchestration: For example, when a user places an order:
                - The frontend-ui microservice might send a request to the api-gateway.
                - The api-gateway might route it to the order-service.
                - The order-service might need to communicate with the inventory-service (to check stock), the user-profile-service (to get shipping details), and then the payment-service (to process payment).

            This inter-service communication is critical for the application to function as a cohesive whole.

    - How Kubernetes Helps: Kubernetes is specifically designed to address these production workload complexities.

        - For Cluster Management & Resource Distribution:
            
            Kubernetes excels at managing a pool of worker nodes (your servers) and distributing your application components (as Pods) across them.
            
            It intelligently schedules workloads based on resource availability and defined constraints, ensuring efficient use of the cluster.
    
            It handles node failures by automatically attempting to reschedule workloads from a failed node to healthy ones, thus maintaining application availability.

        - For Managing Microservices:
            
            Kubernetes is ideally suited for deploying and managing containerized microservices. 
            
            Each microservice can be packaged into a container and then managed by Kubernetes constructs like Deployments.

            This allows for easy, independent scaling of each microservice (e.g., "I need 3 instances of order-service but 10 instances of product-catalog-service").
            
            It facilitates rolling updates and rollbacks for individual microservices, allowing for zero-downtime deployments.

        - For Inter-Service Communication:
            
            Kubernetes provides robust mechanisms for service discovery and internal load balancing through its Service abstraction.

            Each microservice (or set of its Pods) can be exposed via a stable internal IP address and DNS name (e.g., order-service.namespace.svc.cluster.local).

            This means one microservice can reliably find and communicate with another, even as individual Pods (instances of those services) are created, destroyed, or moved across nodes, as their IP addresses are dynamic.

-----

3.) Core Kubernetes Concepts for Application Management:

    - In a microservices environment deployed on Kubernetes, each microservice typically runs in its own set of Pods (managed by a Deployment).
    
    - A Pod can run multiple containers. This is often used for "sidecar" patterns, where a main application container is accompanied by helper/supporting containers (e.g., for logging, monitoring, proxying).

    - ReplicaSet ensures a specified number of Pod replicas are running at any given time. If you state you need 5 replicas of your application, the ReplicaSet works to maintain 5 running Pods.

    - Hierarchy: You define a Deployment -> which creates and manages a ReplicaSet -> which creates and manages Pods.

    - Labels and Selectors:
        
        Labels: Key/value pairs attached to Kubernetes objects (especially Pods).
        
        Selectors: Used to identify and select objects (especially pods) based on their labels.
        
        This is the core mechanism Kubernetes uses to connect different resources. For example, a Deployment uses selectors to know which Pods it's managing, and a Service uses selectors to know which Pods to send traffic to.

    - Namespace provides a way to divide cluster into virtual sub-cluster creating logical isolation within a physical cluster.

    - In microservice env whenever it is deployed on K8S, each microservices should always run in a separate pod / deployment.

-----

4.) Kubernetes Networking: Enabling Communication

Scenario: You have 10 microservices to deploy in a Kubernetes cluster.
- 2 microservices need to be exposed externally to end-users.
- All 10 microservices need to talk to each other within the cluster.

How to achieve this?

Discussion:

    - Pod IPs - The Challenge:
        
        Every Pod gets its own unique IP address within the cluster (viewable with kubectl get pods -n <namespace> -o wide).

        Technically, microservices (running in Pods) could communicate via these Pod IPs.
        
        Problem: Pods are ephemeral. 
        
            If a Pod (e.g., one of 3 checkout-MS Pods) dies, its Deployment/ReplicaSet will create a new one. This new Pod will get a new IP address.
            
            If other microservices (e.g., order-MS) were trying to communicate with the old IP, that communication would fail. Managing these dynamic IPs for inter-service communication is extremely complex and not done in real-world scenarios.

    - Kubernetes Services - The Solution for Stable Communication:
            
            Services provide a stable abstraction (a fixed IP address and DNS name) over a dynamic set of Pods.

            ClusterIP Service (for internal communication): This is the default Service type.
                - Exposes the Service on an internal IP address, only reachable from within the cluster.
                - Acts as an internal load balancer, distributing traffic to the Pods matching its selector.
                - Provides a fixed, stable IP address and a DNS name (e.g., checkout-service.your-namespace.svc.cluster.local) for the service. Microservices use this stable address/name to communicate, regardless of individual Pod IPs changing.


Solution:

    - For our scenario: 
        - The 8 microservices that only need internal communication, and all 10 for inter-service communication, would use ClusterIP Services.

        - The 2 externally exposed microservices could use.:
            - NodePort Service (for basic external access):
                - Exposes the Service on a static port (the NodePort, typically in the 30000-32767 range) on each Worker Node's IP address.
                - Allows external traffic to reach the service via http://<WorkerNodeIP>:<NodePort>.
    
                Example: If your cluster has worker nodes with IPs 112.23.14.57 to 112.23.14.61 and your NodePort is 30080, you can access the service via 112.23.14.57:30080, 112.23.14.58:30080, etc.

            - LoadBalancer Service (for robust external access, usually cloud-provider specific):
                - Exposes the Service externally using a cloud provider's load balancer (e.g., AWS ELB, Azure Load Balancer, GCP Cloud Load Balancer).
                
                - The cloud provider provisions an external load balancer.

        For our scenario: Load Balancer service type is often the preferred method for the 2 externally exposed microservices in a cloud environment.


    - Another option here is: Ingress (for advanced HTTP/S routing):
        - Not a Service type, but an API object that manages external access to services, typically HTTP/S.
        - Provides more advanced features like host-based and path-based routing, SSL/TLS termination.
        - Requires an Ingress Controller (e.g., AWS Load Balancer Controller, Nginx, Traefik, HAProxy) to be running in the cluster.
        
        (details of ingress will be discussed later)
		
----------------------------------------------------------------------------------------------------

Session 2:
-----------

1.) Misconception about Kubernetes Load Balancer:
--------------------------------------------------

Clarification: 
    
    - There isn't a generic "Kubernetes LoadBalancer" object you create directly. Instead, you create a Service of type: LoadBalancer. Kubernetes then interacts with the underlying cloud infrastructure (if available) to provision an actual load balancer.

    - External Access Flow Simplified:
        
        User types https://example.com in his browser.

        Browser queries DNS, which resolves example.com to an IP address. This IP usually points to an external Load Balancer.

        The external Load Balancer forwards traffic into the service running in the Kubernetes cluster 

        The Kubernetes Service inside the cluster then routes the traffic to the correct Pod(s) running the website application.

-----

2.) Configuration Management in Kubernetes

The Need: 

    - Application code often has:
        a.) Business logic.
        b.) Configuration values (database URLs, API keys, feature flags, environment-specific settings).
        c.) The same application code needs to run in different environments (dev, QA, UAT, pre-prod, prod) with different configurations.

    - These configuration files fetch the configuration values from:
        - Env variables
        - External env specific config file, dev-config.yaml, prod-config.yaml
        e.g.: environment-dev.ts, environment-prod.ts

Kubernetes Solutions:

    - ConfigMap: 
        
        Stores non-confidential configuration data as key-value pairs (e.g., database server URL, DB username, service names to connect to).
        
        These can be injected into Pods as environment variables or mounted as files.

    - Secret: 
    
        Stores sensitive data like passwords, API tokens, TLS certificates. Data is stored base64 encoded by default (offering mild obfuscation, not strong encryption at rest unless additional cluster security measures are in place). 
        
        Secrets can also be injected as environment variables or mounted as files.

Important Note:
    
    - A ConfigMap is for injecting configuration data. It cannot directly read files from a Persistent Volume (PV).
    
    - Pods are the entities that perform OS-level operations like reading/writing files. 
    
    - A Pod can mount a ConfigMap (as files) and also mount a PersistentVolumeClaim (PVC, which uses a PV) to access persistent storage.

-----

3.) Scaling Applications in Kubernetes:

    - Manual Scaling: Directly changing the replicas count in a Deployment manifest (e.g., kubectl scale deployment my-app --replicas=10 or changing the replica count in the deployment ).

    - Autoscaling (Dynamic):
        
        -  Horizontal Pod Autoscaler (HPA):

            Automatically scales the number of Pod replicas in a Deployment or ReplicaSet.
            
            Scaling decisions are based on observed metrics like CPU utilization, memory usage, or custom metrics.


        - Vertical Pod Autoscaler (VPA):
            
            Automatically adjusts the CPU and memory resource requests and limits for Pods within a Deployment. (Note: This often involves restarting Pods to apply new resource settings, so it's used more carefully).


4.) Sample Use case: End to end scaling (including pods and worker noes in the cluster)

    - Setup:
        Cluster: 1 control plane, 3 worker nodes.
        Worker node capacity (example): 2 vCPUs, 8GB RAM, 30GB Storage each.
        Application APP10 deployed.
        HPA for APP10: min replicas=2, max replicas=10, target CPU utilization=75%.

    - Event: High traffic hits APP10, average CPU usage across Pods exceeds 75%.
        HPA Action: HPA starts scaling up Pods from 2 towards 10.

    - Problem: Let's say HPA successfully creates 8 new Pods (total 10), but the 9th and 10th Pods go into a Pending state.

    - Reason: Upon checking Pod events (kubectl describe pod <pending-pod-name>), you find messages like "insufficient cpu" or "insufficient memory." The Kubernetes scheduler cannot find any worker nodes with enough available (unallocated) CPU or memory to place these new Pods. The cluster has run out of allocatable resources on the existing nodes.

    - Cluster scaling Solutions:
        Add more worker nodes to the cluster (potentially automated by a Cluster Autoscaler and cloud based auto-scaling solution if on-cloud).
		
----------------------------------------------------------------------------------------------------
	
Session 3:
-----------

1.) Self-Managed Kubernetes Cluster Architecture

	- When you're not using a managed Kubernetes service like EKS, GKE, or AKS, you are responsible for setting up and maintaining all components of the Kubernetes cluster.

	- Control Plane (Master Nodes):
		The brain of the Kubernetes cluster. It makes decisions about the cluster (e.g., scheduling) and detects and responds to cluster events. For production and critical use cases, you may also run multiple control plane nodes / servers for high availability.
		
		- Key Components (running on Control Plane nodes):
			- kube-apiserver: The frontend for the Kubernetes control plane. It exposes the Kubernetes API. All interactions with the cluster (from kubectl, other components, or external tools) go through the API server.

			- etcd: A consistent and highly-available key-value store used as Kubernetes backing store for all cluster data (configurations, state, secrets, etc.). This is critical and losing etcd data means losing the state of your cluster.

			- kube-scheduler: Watches for newly created Pods that have no node assigned and selects a node for them to run on based on resource requirements, policies, and other specifications, etc.

			- controller-manager: Runs controller processes. Controllers are control loops that watch the shared state of the cluster through the apiserver and make changes attempting to move the current state towards the desired state. 
				
				- Examples include:
					- Node Controller: Responsible for noticing and responding when nodes go down.
					 
					- Replication Controller (and ReplicaSet Controller via Deployments): Responsible for maintaining the correct number of pods for every replication controller object in the system.

					- Endpoints Controller: Populates the Endpoints object (i.e., joins Services & Pods).
			
			- cloud-controller-manager (Optional, for cloud provider integration): Embeds cloud-specific control logic. It allows you to link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that just interact with your cluster. (e.g., managing load balancers, storage volumes). We will not need this in our self managed cluster.
			
	- Worker Nodes:
		These are the machines (VMs or physical servers) where your actual application containers (in Pods) run.

		- Key Components (running on each Worker Node):
			- kubelet: An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod as described by their PodSpecs received from the API server running on the Control Plane. It registers the node with the apiserver.

			- kube-proxy: kube-proxy makes Kubernetes Services (like ClusterIP) work by creating network rules on every node. These rules ensure that when you try to reach a Service's stable IP address, your request gets directed to one of the actual, healthy Pods providing that service.
			
			- Container Runtime Interface (CRI): The software responsible for running containers. Kubernetes is CRI-compatible, meaning you can use various container runtimes.
			Examples: containerd, Docker (via dockershim - being deprecated as a direct kubelet integration), CRI-O.

			- Container Networking Interface (CNI) Plugin: CNI plugins give each Pod its own IP address and set up the network paths so Pods can talk directly to each other, even across different machines. They are essential for basic Pod-to-Pod communication.

-----

2.) Infrastructure Setup for Self-Managed Cluster:
	
	- Virtual Machines (VMs): You will typically provision VMs. 
		For example:
			- 1 or 3 VMs for the Control Plane (odd number for etcd quorum).
			- At least 2 or more VMs for Worker Nodes for basic redundancy and capacity.

	- Networking:

		- VPC / VNet / Intranet: All VMs (control plane and worker nodes) must reside within the same private network (e.g., AWS VPC, Azure VNet, or a corporate intranet) allowing them to communicate with each other.

		- Security Groups (SG) / Network Security Groups (NSG) / Firewalls: These are crucial cloud (or on-prem) constructs that act as virtual firewalls for your VMs. You need to configure rules to:
			- Allow control plane components to talk to each other (especially etcd).
			- Allow worker nodes (kubelet, kube-proxy) to communicate with the kube-apiserver on the control plane.
			- Allow inter-node communication for the CNI plugin (Pod-to-Pod networking overlay).
			- Allow external access to the kube-apiserver for kubectl (usually restricted to specific IPs).
			- Allow inbound traffic for services exposed via NodePort or LoadBalancer.

	- Installation of Kubernetes Components:
		
		- kubeadm: A command-line tool that provides kubeadm init and kubeadm join as best-practice "fast paths" for creating Kubernetes clusters. It bootstraps the control plane and worker nodes by installing and configuring the necessary components. This is the most common method for self-managed clusters.

		- Manifests / Helm: While core Kubernetes components are typically installed by tools like kubeadm or by cloud providers, you use YAML manifests or Helm charts to deploy applications and add-ons (like monitoring tools, ingress controllers, or CNI plugins if not bundled) onto your running cluster.

----------------------------------------------------------------------------------------------------

Session 4:
-----------

1.) Networking Layers: L4 vs L7 Load Balancing

	- Understanding the difference is key to choosing the right type of Kubernetes Service or Ingress.
		- Layer 4 (Transport Layer - TCP/UDP):
			- Operation: Works at the TCP or UDP protocol level. It makes routing decisions based on source/destination IP addresses and ports.
			- Visibility: It does not inspect the actual content (payload) of the network packets. It doesn't understand HTTP headers, paths, or cookies.
			
			- Use Cases: 
				- Simple, fast load balancing for any TCP/UDP based service. Good for high throughput, low latency scenarios where content inspection is not needed.

			- Kubernetes/Cloud Examples:
				- Kubernetes Service of type LoadBalancer (when provisioned by a cloud provider like Azure Standard Load Balancer or AWS Network Load Balancer (NLB)).
				- kube-proxy itself in IPVS mode or iptables mode largely operates at L4 for Service routing.

		- Layer 7 (Application Layer - HTTP/HTTPS, gRPC, etc.):
			- Operation: Works at the application protocol level (e.g., HTTP). It can inspect the content of the messages.
			- Visibility: Can read and understand application-level data like:
				- HTTP Host headers (e.g., Host: flipkart.com)
				- HTTP Paths (e.g., /product, /user/profile)
				- HTTP Methods (GET, POST)
				- Cookies, Query parameters
				- Other headers

			- Use Cases: 
				- Content-based routing / Path-based routing: Directing traffic to different backend services based on the URL path (e.g., example.com/api/* to API service, example.com/ui/* to UI service).
				- Host-based routing: Directing traffic based on the domain name (e.g., app1.example.com to app1 service, app2.example.com to app2 service).
				
			- Kubernetes/Cloud Examples:
				- Kubernetes Ingress controllers (Nginx Ingress, Traefik, Istio Gateway).
				- Cloud-native L7 Load Balancers like AWS Application Load Balancer (ALB), Azure Application Gateway, Google Cloud HTTP(S) Load Balancer.

2.) Pods in EKS Accessing AWS Services (The IRSA Deep Dive)

	- If your Pods running inside an Amazon EKS (Elastic Kubernetes Service) cluster need to access AWS services (like S3, DynamoDB, SQS, or even make API calls to create ALBs via the AWS Load Balancer Controller), they need AWS credentials.

	- Option 1: Worker Node IAM Role (The Less Secure, Legacy Approach)
		- How it works: You create an IAM Role and attach it to the EKS Cluster Node Group that serve as your EKS worker nodes. This role has policies granting permissions to various AWS services.
		- The kubelet on the worker node (and thus any Pod running on that node) can then inherit these permissions by querying the EC2 instance metadata service.

		- Drawbacks:
			- Overly Permissive: All Pods running on a particular node get all the permissions granted by the node's IAM role. This violates the principle of least privilege. A compromised Pod could potentially abuse all permissions of the node.
			- Difficult to Audit: Hard to track which specific Pod/application used which permission.

	- Option 2: IRSA - IAM Roles for Service Accounts (The Preferred, Secure Approach)
		- IRSA allows you to associate an AWS IAM Role directly with a Kubernetes Service Account. Pods that use this Service Account can then assume the associated IAM Role and get fine-grained, specific permissions.

		- Core Concepts & Setup Flow:
			- EKS Cluster OIDC Provider:
				(check the AWS Load Balancer Controller installation Guide to perform the steps, read this for the explanation)
				- Your EKS cluster has an OpenID Connect (OIDC) issuer URL. This URL acts as an identity provider (IdP).
				- You need to enable this in your EKS cluster if not already done. This OIDC provider issues tokens that AWS STS (Security Token Service) can verify.
				- Find it via EKS console.
				- Create an IAM OIDC Identity Provider in AWS IAM
				- Create an IAM Role with a Trust Policy for the OIDC Provider, this is the IAM Role your Pods will assume.
				- Permissions Policy: Attach policies that grant the specific, minimal permissions your application (running in the Pod) needs (e.g., s3:GetObject on a specific bucket).
				- Trust Relationship (Crucial Part - "Web Identity Trusted Entity"):
					- The "Principal" in the trust policy will be the ARN of the OIDC provider created above
					- It uses a Condition to scope down which Kubernetes Service Account can assume this role, 
							- The :sub condition ensures only tokens issued for that specific Service Account can assume the role.
							- The :aud condition ensures the token was intended for STS.
				- Create a Kubernetes Service Account (KSA):
					- If it doesn't exist, create it in your desired namespace:
						- kubectl create serviceaccount YOUR_K8S_SERVICE_ACCOUNT_NAME -n YOUR_NAMESPACE
				- Annotate the Kubernetes Service Account:
					- Link the KSA to the IAM Role by adding an annotation:
						- kubectl annotate serviceaccount YOUR_K8S_SERVICE_ACCOUNT_NAME -n YOUR_NAMESPACE eks.amazonaws.com/role-arn=arn:aws:iam::ACCOUNT_ID:role/YOUR_IAM_ROLE_NAME
				- Configure Your Pod/Deployment to Use the Service Account by specifying 
					- serviceAccountName: YOUR_K8S_SERVICE_ACCOUNT_NAME # This should be done in the deployment manifest file

		- How IRSA is Used in AWS Load Balancer Controller:
			- You create an IAM Role (e.g., AmazonEKSLoadBalancerControllerRole) with the necessary policies (AWS provides a recommended policy).
			- You configure its trust policy to allow the Service Account used by the AWS Load Balancer Controller Pods to assume this role (following the IRSA trust policy structure).
			- When you deploy the AWS Load Balancer Controller (e.g., via Helm), you specify the Service Account name, and this Service Account is created and then annotated with the ARN of AmazonEKSLoadBalancerControllerRole.
			- The controller Pods then use IRSA to obtain temporary AWS credentials, allowing them to make API calls to AWS to manage ALBs/NLBs on your behalf.

----------------------------------------------------------------------------------------------------

Session 5:
-----------

1.) Helm:

	- The Problem: Managing Kubernetes Applications Across Environments

		- When we start deploying our microservices to Kubernetes, we begin by writing manifest files. A typical microservice requires a set of these files to describe its desired state.

		- Manifests per Microservice:

			- deployment.yaml: Defines the Pod specification, number of replicas, and update strategy.

			- service.yaml: Exposes the application pods as a stable network service.

			- configmap.yaml: Manages non-sensitive configuration data, like API endpoints or feature flags.

			- secret.yaml: Manages sensitive data like database passwords or API keys.

			- persistentvolumeclaim.yaml: Requests stateful storage for databases or file uploads.

		- This process works well for a single deployment into a development (Dev) environment. However, real-world software delivery requires promoting applications through multiple stages.

		- Standard Delivery Pipeline:

			- For example: Dev Env -> QA Env -> UAT Env -> Production Env

	- The Challenge: Environment-Specific Configuration

		- The core problem is that the Kubernetes manifests are not identical across these environments. Each environment has unique configuration requirements.

		- Examples of Environmental Differences:

			- Replica Count: Dev might have 1 replica, while Prod needs 10+ for high availability.

			- Image Tags: Dev uses feature-branch images (feature-new-login-xyz), while Prod must only use stable, tagged releases (v2.5.1).

			- Configuration (ConfigMaps): The database hostname for Dev (dev-db.internal) is different from Prod (prod-rds-instance.aws.com).

			- Secrets: Each environment has its own unique database passwords and API keys.

			- Resource Limits: Prod applications require significantly more CPU and memory allocated than Dev applications.

		- This leads to a critical question: How do we manage these manifest variations without creating a maintenance overhead?

			- Common (but Flawed) Approaches:

				- Separate Folders: Creating dev/, qa/, and prod/ folders, each with a full copy of the manifests.

					- Problem: This leads to massive duplication. A simple change to a shared label or annotation must be manually applied to every folder, which is slow and extremely error-prone.

				- CI/CD Scripting: Using tools like Jenkins to find-and-replace placeholders in template files.

					- Problem: This makes your CI/CD pipeline complex and brittle. The logic for your application's configuration is now hidden inside scripts instead of being declaratively managed in a repository.

		- So a need for Kubernetes-native solution designed for packaging applications and managing their configurations is clearly identified.

	- Solution: Helm - The Package Manager for Kubernetes

		- Helm solves the packaging and configuration problem.

		- Helm is a tool that streamlines installing and managing Kubernetes applications. It uses a packaging format called charts.

		- Core Helm Concepts:

			- Chart: A Helm package. It's a collection of files in a directory that describes a related set of Kubernetes resources. This is where we create our templates.

			- Values: A file (typically values.yaml) that provides the default configuration for a chart. These are the values we will override for each environment (values-dev.yaml or values-prod.yaml etc.)

			- Release: An instance of a chart running in a Kubernetes cluster. A single chart can be installed multiple times, creating unique releases.

		- How Helm Solves Our Problem

			- With Helm, we create one chart for our microservice. Instead of hardcoding values, we use placeholders.

				- Example: Templated deployment.yaml

						apiVersion: apps/v1
						kind: Deployment
						metadata:
						  name: {{ .Release.Name }}-deployment
						spec:
						  replicas: {{ .Values.replicaCount }} # Placeholder for replica count
						  template:
							spec:
							  containers:
								- name: my-app
								  image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}" # Placeholders
								  env:
									- name: DB_HOST
									  value: {{ .Values.config.databaseHost }}


				- Now, we manage the differences with small, environment-specific values files.

					- Example: Environment-Specific Value Files

						- values-dev.yaml
						
								replicaCount: 1
								image:
								  tag: "feature-xyz"
								config:
								  databaseHost: "dev-db.internal"

						- values-prod.yaml

								replicaCount: 10
								image:
								  tag: "v2.5.1"
								config:
								  databaseHost: "prod-rds-instance.aws.com"

				- Deployment Command:

					- To deploy to production, we combine the chart with the production values file.

						- helm upgrade --install my-app ./my-chart -f values-prod.yaml

					- To deploy to dev, we combine the chart with the dev values file.

						- helm upgrade --install my-app ./my-chart -f values-dev.yaml

	- Key Benefits:

		- DRY (Don't Repeat Yourself): The application's structure is defined once. Only the configuration values change.

		- Single Command Operations: Deploy or upgrade an entire multi-resource application in one shot.

		- Reliable Rollbacks: If a new release is faulty, you can instantly roll back to a previously deployed version with helm rollback <release-name>.

----------------------------------------------------------------------------------------------------

Session 6:
-----------

1.) A Better Process: GitOps - Git as the Single Source of Truth

	- Helm solved our packaging problem. GitOps solves our deployment process problem.

	- GitOps is an operational framework that uses a Git repository as the single source of truth for declarative infrastructure and applications. 
	
	- The goal is to automate the deployment process by ensuring that the live state of your cluster always matches the state defined in Git.

	- Core Principles of GitOps:

		- Declarative: The entire desired state of your system is described declaratively in a Git repository (e.g., using Helm charts or plain YAML).

		- Versioned and Immutable: Git is the source of truth, giving you a complete audit trail of all changes, the ability to revert changes, and clear ownership.

		- Pulled Automatically: An automated agent in the cluster pulls the desired state from Git and applies it.

		- Continuously Reconciled: The agent continuously observes the cluster's state and corrects any drift from the state defined in Git.

2.) The Tool: ArgoCD - A Declarative GitOps Controller

	- ArgoCD is the agent that lives in your cluster and makes the GitOps workflow a reality.

	- How ArgoCD Works:

		- You configure an ArgoCD Application resource, telling it which Git repository to monitor.

		- ArgoCD fetches the manifests from the Git repository.

		- It compares the state defined in Git (the "desired state") with the resources actually running in the cluster (the "live state").

		- If there is a difference, ArgoCD reports the application as OutOfSync.

		- It can then automatically (or with a manual click) synchronize the cluster, applying the necessary changes to match the state in Git.